{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alok8663/Deep_Learning/blob/main/Unsupervised_Deep_Learning/AutoEncoders.ipynb)\n"
      ],
      "metadata": {
        "id": "AZ9YrEI32dCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the libraries"
      ],
      "metadata": {
        "id": "68R-OthL2Wn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUcEtsuqaNx0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset\n"
      ],
      "metadata": {
        "id": "RvaPGPhS2UPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies=pd.read_csv('/content/drive/MyDrive/P16-AutoEncoders/AutoEncoders/ml-1m/ml-1m/movies.dat',sep='::',header=None,engine='python',encoding='latin-1')\n",
        "users=pd.read_csv('/content/drive/MyDrive/P16-AutoEncoders/AutoEncoders/ml-1m/ml-1m/users.dat',sep='::',header=None,engine='python',encoding='latin-1')\n",
        "ratings=pd.read_csv('/content/drive/MyDrive/P16-AutoEncoders/AutoEncoders/ml-1m/ml-1m/ratings.dat',sep='::',header=None,engine='python',encoding='latin-1')"
      ],
      "metadata": {
        "id": "GkOYF58mbHZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(movies)"
      ],
      "metadata": {
        "id": "zn36V1YBehKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(users)"
      ],
      "metadata": {
        "id": "Rq7917AufPWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(ratings)"
      ],
      "metadata": {
        "id": "Uj3BpQ5lgjI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the training set and the test set\n"
      ],
      "metadata": {
        "id": "_iUwbP2R2OWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=pd.read_csv('/content/drive/MyDrive/P16-AutoEncoders/AutoEncoders/ml-100k/ml-100k/u1.base',delimiter='\\t')\n",
        "training_set=np.array(training_set,dtype='int')\n",
        "test_set=pd.read_csv('/content/drive/MyDrive/P16-AutoEncoders/AutoEncoders/ml-100k/ml-100k/u1.test',delimiter='\\t')\n",
        "test_set=np.array(test_set,dtype='int')"
      ],
      "metadata": {
        "id": "rw88dQQBjrHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(training_set))"
      ],
      "metadata": {
        "id": "_qd_eEh498wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(test_set))"
      ],
      "metadata": {
        "id": "yTowCycT_kLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the number of users and movies\n"
      ],
      "metadata": {
        "id": "0-tB6Cni2J4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_users=int(max(max(training_set[:,0]),max(test_set[:,0])))\n",
        "nb_movies=int(max(max(training_set[:,1]),max(test_set[:,1])))"
      ],
      "metadata": {
        "id": "92WAbaDckCDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nb_users)"
      ],
      "metadata": {
        "id": "NUQ4EMTH9kCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nb_movies)"
      ],
      "metadata": {
        "id": "xNOrCHY_-aNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the data into an array with users in lines and movies in columns\n"
      ],
      "metadata": {
        "id": "5Xz0fcE91_4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(data):\n",
        "  new_data=[]\n",
        "  for id_users in range(1,nb_users+1):\n",
        "    id_movies=data[:,1][data[:,0]==id_users]\n",
        "    id_ratings=data[:,2][data[:,0]==id_users]\n",
        "    ratings=np.zeros(nb_movies)\n",
        "    ratings[id_movies-1]=id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data\n",
        "training_set=convert(training_set)\n",
        "test_set=convert(test_set)"
      ],
      "metadata": {
        "id": "Ey3IpzPF1N8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the data into Torch tensors\n"
      ],
      "metadata": {
        "id": "7Yxk-QKh8G_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set=torch.FloatTensor(training_set)\n",
        "test_set=torch.FloatTensor(test_set)"
      ],
      "metadata": {
        "id": "uZwKZDEk6asw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the architecture of the Neural Network\n"
      ],
      "metadata": {
        "id": "WTS37VNU_u-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "metadata": {
        "id": "oQxwYuZPuNYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the SAE\n"
      ],
      "metadata": {
        "id": "8geOomGfufvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.0\n",
        "    for id_user in range(nb_users):\n",
        "        input = training_set[id_user].unsqueeze(0)\n",
        "        target = input.clone().detach()\n",
        "        if torch.sum(target > 0) > 0:\n",
        "            output = sae(input)\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies / float(torch.sum(target > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "            s += 1.0\n",
        "        optimizer.step()\n",
        "    print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss / s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qttJqWxqucgG",
        "outputId": "59c2af84-47a1-44a2-e56d-4ccd2ce824c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 loss: 1.7713223356454868\n",
            "epoch: 2 loss: 1.0966304928315376\n",
            "epoch: 3 loss: 1.0535944978208638\n",
            "epoch: 4 loss: 1.0384002321124979\n",
            "epoch: 5 loss: 1.0309284818712234\n",
            "epoch: 6 loss: 1.0263672901386272\n",
            "epoch: 7 loss: 1.0237764360146901\n",
            "epoch: 8 loss: 1.0218855034994003\n",
            "epoch: 9 loss: 1.0208011875316014\n",
            "epoch: 10 loss: 1.0196094550490975\n",
            "epoch: 11 loss: 1.018809610438656\n",
            "epoch: 12 loss: 1.0183963817306936\n",
            "epoch: 13 loss: 1.0177505257752046\n",
            "epoch: 14 loss: 1.0174010313866118\n",
            "epoch: 15 loss: 1.0172639032233477\n",
            "epoch: 16 loss: 1.016886197304884\n",
            "epoch: 17 loss: 1.0165491029277773\n",
            "epoch: 18 loss: 1.0163684886184365\n",
            "epoch: 19 loss: 1.016356834800117\n",
            "epoch: 20 loss: 1.0159727307975073\n",
            "epoch: 21 loss: 1.0161603975969502\n",
            "epoch: 22 loss: 1.0159111838081905\n",
            "epoch: 23 loss: 1.0160097242021031\n",
            "epoch: 24 loss: 1.015989761434032\n",
            "epoch: 25 loss: 1.015707202161073\n",
            "epoch: 26 loss: 1.0153073331566524\n",
            "epoch: 27 loss: 1.0152640724748483\n",
            "epoch: 28 loss: 1.015080468922573\n",
            "epoch: 29 loss: 1.0129257474202653\n",
            "epoch: 30 loss: 1.0110005727523081\n",
            "epoch: 31 loss: 1.0102698191896493\n",
            "epoch: 32 loss: 1.0068405388213741\n",
            "epoch: 33 loss: 1.0069666750055382\n",
            "epoch: 34 loss: 1.0019702972954398\n",
            "epoch: 35 loss: 1.000531925958092\n",
            "epoch: 36 loss: 0.9983442594151193\n",
            "epoch: 37 loss: 0.9983438049674361\n",
            "epoch: 38 loss: 0.9951524528041585\n",
            "epoch: 39 loss: 0.9945974386759631\n",
            "epoch: 40 loss: 0.9899563091938637\n",
            "epoch: 41 loss: 0.9888554597408105\n",
            "epoch: 42 loss: 0.9855428196319473\n",
            "epoch: 43 loss: 0.9851564134120987\n",
            "epoch: 44 loss: 0.9810669848765459\n",
            "epoch: 45 loss: 0.9824890753972039\n",
            "epoch: 46 loss: 0.9777077426280257\n",
            "epoch: 47 loss: 0.9765587816687619\n",
            "epoch: 48 loss: 0.970282262861259\n",
            "epoch: 49 loss: 0.9709889967557941\n",
            "epoch: 50 loss: 0.9725647107393557\n",
            "epoch: 51 loss: 0.9685373850801015\n",
            "epoch: 52 loss: 0.9645365063437754\n",
            "epoch: 53 loss: 0.9630372606501442\n",
            "epoch: 54 loss: 0.9608772132033719\n",
            "epoch: 55 loss: 0.959994414236295\n",
            "epoch: 56 loss: 0.9572322436929245\n",
            "epoch: 57 loss: 0.9577296357564662\n",
            "epoch: 58 loss: 0.9541045547760864\n",
            "epoch: 59 loss: 0.9542794493519335\n",
            "epoch: 60 loss: 0.9525997709123077\n",
            "epoch: 61 loss: 0.9526013777534666\n",
            "epoch: 62 loss: 0.9504532351489197\n",
            "epoch: 63 loss: 0.9506547714279032\n",
            "epoch: 64 loss: 0.948546661180957\n",
            "epoch: 65 loss: 0.9485127287312247\n",
            "epoch: 66 loss: 0.9470135403206802\n",
            "epoch: 67 loss: 0.9471293071848018\n",
            "epoch: 68 loss: 0.9459214543050136\n",
            "epoch: 69 loss: 0.9452201598598124\n",
            "epoch: 70 loss: 0.9444076273672912\n",
            "epoch: 71 loss: 0.9447742883922264\n",
            "epoch: 72 loss: 0.9431245689125652\n",
            "epoch: 73 loss: 0.9438398104373741\n",
            "epoch: 74 loss: 0.9421207590900296\n",
            "epoch: 75 loss: 0.942225831647896\n",
            "epoch: 76 loss: 0.9406018392181382\n",
            "epoch: 77 loss: 0.9409954397376933\n",
            "epoch: 78 loss: 0.9396882674727864\n",
            "epoch: 79 loss: 0.9401114699088254\n",
            "epoch: 80 loss: 0.9384548691900229\n",
            "epoch: 81 loss: 0.9387429562122698\n",
            "epoch: 82 loss: 0.9376617031838637\n",
            "epoch: 83 loss: 0.9375898785829001\n",
            "epoch: 84 loss: 0.9368296346986185\n",
            "epoch: 85 loss: 0.9369412083968708\n",
            "epoch: 86 loss: 0.9359291880639279\n",
            "epoch: 87 loss: 0.936052745844312\n",
            "epoch: 88 loss: 0.9352967464591853\n",
            "epoch: 89 loss: 0.9354371415442215\n",
            "epoch: 90 loss: 0.9346217915209994\n",
            "epoch: 91 loss: 0.9348499968993316\n",
            "epoch: 92 loss: 0.9337877136297906\n",
            "epoch: 93 loss: 0.9341064664032179\n",
            "epoch: 94 loss: 0.9333690819645037\n",
            "epoch: 95 loss: 0.9336821253002112\n",
            "epoch: 96 loss: 0.9322169100459413\n",
            "epoch: 97 loss: 0.933106935694915\n",
            "epoch: 98 loss: 0.9317708781557646\n",
            "epoch: 99 loss: 0.9317951893477286\n",
            "epoch: 100 loss: 0.9314033826547955\n",
            "epoch: 101 loss: 0.9315820177562003\n",
            "epoch: 102 loss: 0.9312241436816394\n",
            "epoch: 103 loss: 0.9311528225747819\n",
            "epoch: 104 loss: 0.9302622927337779\n",
            "epoch: 105 loss: 0.9304241261986012\n",
            "epoch: 106 loss: 0.9295530792177771\n",
            "epoch: 107 loss: 0.9297602704892381\n",
            "epoch: 108 loss: 0.9290846183573579\n",
            "epoch: 109 loss: 0.9290285181342998\n",
            "epoch: 110 loss: 0.9284320582519368\n",
            "epoch: 111 loss: 0.9283268008867145\n",
            "epoch: 112 loss: 0.9276953980021163\n",
            "epoch: 113 loss: 0.9277660195913786\n",
            "epoch: 114 loss: 0.9271873890785925\n",
            "epoch: 115 loss: 0.927220862361981\n",
            "epoch: 116 loss: 0.9264360881434079\n",
            "epoch: 117 loss: 0.9265275216430865\n",
            "epoch: 118 loss: 0.9261459044752328\n",
            "epoch: 119 loss: 0.9260180482757986\n",
            "epoch: 120 loss: 0.9254232010660812\n",
            "epoch: 121 loss: 0.9251498631394687\n",
            "epoch: 122 loss: 0.9246606954688283\n",
            "epoch: 123 loss: 0.9248826294729556\n",
            "epoch: 124 loss: 0.924294991374636\n",
            "epoch: 125 loss: 0.9242360613585311\n",
            "epoch: 126 loss: 0.9237919539285908\n",
            "epoch: 127 loss: 0.9240857001703022\n",
            "epoch: 128 loss: 0.9230050959636755\n",
            "epoch: 129 loss: 0.9232823916453715\n",
            "epoch: 130 loss: 0.9227024976206262\n",
            "epoch: 131 loss: 0.9227739017517906\n",
            "epoch: 132 loss: 0.9221942570709808\n",
            "epoch: 133 loss: 0.922239191353768\n",
            "epoch: 134 loss: 0.9218635474794373\n",
            "epoch: 135 loss: 0.9220846782678275\n",
            "epoch: 136 loss: 0.9215206854444805\n",
            "epoch: 137 loss: 0.9217157412023173\n",
            "epoch: 138 loss: 0.9211253565715865\n",
            "epoch: 139 loss: 0.920849059988015\n",
            "epoch: 140 loss: 0.9205888352625088\n",
            "epoch: 141 loss: 0.9204794268262698\n",
            "epoch: 142 loss: 0.9206309245488022\n",
            "epoch: 143 loss: 0.9203481007035996\n",
            "epoch: 144 loss: 0.9195884624514326\n",
            "epoch: 145 loss: 0.9197447394497591\n",
            "epoch: 146 loss: 0.9193704874117969\n",
            "epoch: 147 loss: 0.919001850282276\n",
            "epoch: 148 loss: 0.9185825682634974\n",
            "epoch: 149 loss: 0.9185705549601925\n",
            "epoch: 150 loss: 0.9183181393336438\n",
            "epoch: 151 loss: 0.9184100551560609\n",
            "epoch: 152 loss: 0.9180529183888426\n",
            "epoch: 153 loss: 0.9182589001867723\n",
            "epoch: 154 loss: 0.9177122617294008\n",
            "epoch: 155 loss: 0.9179050330489695\n",
            "epoch: 156 loss: 0.9175015014698146\n",
            "epoch: 157 loss: 0.9173667432065308\n",
            "epoch: 158 loss: 0.9171291220038076\n",
            "epoch: 159 loss: 0.9170284177248405\n",
            "epoch: 160 loss: 0.9167868403383728\n",
            "epoch: 161 loss: 0.9165930551087508\n",
            "epoch: 162 loss: 0.9162005997917905\n",
            "epoch: 163 loss: 0.9163061927474064\n",
            "epoch: 164 loss: 0.9161087253027654\n",
            "epoch: 165 loss: 0.9159980865774778\n",
            "epoch: 166 loss: 0.915601770209881\n",
            "epoch: 167 loss: 0.9158414367519339\n",
            "epoch: 168 loss: 0.915519535364302\n",
            "epoch: 169 loss: 0.9153317599485817\n",
            "epoch: 170 loss: 0.9151713517165277\n",
            "epoch: 171 loss: 0.9151198197327711\n",
            "epoch: 172 loss: 0.9148918263477922\n",
            "epoch: 173 loss: 0.914922007943523\n",
            "epoch: 174 loss: 0.9146056364402844\n",
            "epoch: 175 loss: 0.914743598254232\n",
            "epoch: 176 loss: 0.9144309561026001\n",
            "epoch: 177 loss: 0.9145296487627588\n",
            "epoch: 178 loss: 0.9141525431856302\n",
            "epoch: 179 loss: 0.9142148383452621\n",
            "epoch: 180 loss: 0.9140173612716476\n",
            "epoch: 181 loss: 0.9138656081152629\n",
            "epoch: 182 loss: 0.9134916443637158\n",
            "epoch: 183 loss: 0.9135525490255887\n",
            "epoch: 184 loss: 0.9132263885565217\n",
            "epoch: 185 loss: 0.9133058610205833\n",
            "epoch: 186 loss: 0.9127577512399127\n",
            "epoch: 187 loss: 0.9130401769266363\n",
            "epoch: 188 loss: 0.9124017662410017\n",
            "epoch: 189 loss: 0.9125508541454339\n",
            "epoch: 190 loss: 0.9123382424062652\n",
            "epoch: 191 loss: 0.9126131890921536\n",
            "epoch: 192 loss: 0.9121818174431316\n",
            "epoch: 193 loss: 0.9122212514225546\n",
            "epoch: 194 loss: 0.9119445134206007\n",
            "epoch: 195 loss: 0.9118675628563545\n",
            "epoch: 196 loss: 0.9116410724879803\n",
            "epoch: 197 loss: 0.9117273058262784\n",
            "epoch: 198 loss: 0.9114354380192733\n",
            "epoch: 199 loss: 0.9113920650211491\n",
            "epoch: 200 loss: 0.9110665618019269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the SAE\n",
        "\n"
      ],
      "metadata": {
        "id": "rKmehO1E2-d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0\n",
        "s = 0.0\n",
        "for id_user in range(nb_users):\n",
        "    input = training_set[id_user].unsqueeze(0)\n",
        "    target = test_set[id_user].unsqueeze(0).clone().detach()\n",
        "    if torch.sum(target > 0) > 0:\n",
        "        output = sae(input)\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies / float(torch.sum(target > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "        s += 1.0\n",
        "print('test loss: ' + str(test_loss / s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH5ueHE229yO",
        "outputId": "233e5665-a36b-473c-d7f5-9179c9403776"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.9518670224530947\n"
          ]
        }
      ]
    }
  ]
}